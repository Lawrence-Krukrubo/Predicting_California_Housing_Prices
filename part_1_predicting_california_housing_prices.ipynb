{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part_1_predicting_california_housing_prices.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHziMANnwhY99vlgQW04J/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lawrence-Krukrubo/Predicting_California_Housing_Prices/blob/master/part_1_predicting_california_housing_prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keq1uqwC6CS4",
        "colab_type": "text"
      },
      "source": [
        "We shall hard-code a Neural Network using python and use it to predict the price of houses in california. <br> Each hidden layer and output layer will have the Relu activation function applied.\n",
        "\n",
        "See this [link for reference](https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7o1dvcp88Xg",
        "colab_type": "text"
      },
      "source": [
        "The [Universal Function Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that a neural network with a single hidden layer and a finite number of neurons can approximate continous functions on compact subsets of data points in hyper-dimensional vector spaces, under mild assumptions of the activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgSV_gAkj_pF",
        "colab_type": "text"
      },
      "source": [
        "## **PART 1: The Needed Modules:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-_XA1zyxTTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fe08e8a-f94f-4bb3-d596-d99c477b0111"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "import sklearn.linear_model\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "print('all modules imported!')"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all modules imported!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2LRj5F1KpV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsAnPZGKkL7_",
        "colab_type": "text"
      },
      "source": [
        "## **PART 2: The Data and Preprocessing:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfBj1-Jox4N0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9f10c626-cbc2-4862-9cdd-5e6a3c0ccd19"
      },
      "source": [
        "california_train = pd.read_csv('sample_data/california_housing_train.csv')\n",
        "california_test = pd.read_csv('sample_data/california_housing_test.csv')\n",
        "print(f'Shape of training data is: {california_train.shape},\\nShape of testing data is: {california_test.shape}')"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training data is: (17000, 9),\n",
            "Shape of testing data is: (3000, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XOrxMDxJpGz",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>The Data Dictionary:</b></h4>\n",
        "\n",
        "1. **longitude:** <br>A measure of how far west a house is; a higher value is farther west\n",
        "\n",
        "2. **latitude:** <br>A measure of how far north a house is; a higher value is farther north\n",
        "\n",
        "3. **housingMedianAge:** <br>Median age of a house within a block; a lower number is a newer building\n",
        "\n",
        "4. **totalRooms:** <br>Total number of rooms within a block\n",
        "\n",
        "5. **totalBedrooms:** <br>Total number of bedrooms within a block\n",
        "\n",
        "6. **population:** <br>Total number of people residing within a block\n",
        "\n",
        "7. **households:** <br>Total number of households, a group of people residing within a home unit, for a block\n",
        "\n",
        "8. **medianIncome:** <br>Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
        "\n",
        "9. **medianHouseValue:** <br>Median house value for households within a block (measured in US Dollars)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzuy-eADKgjo",
        "colab_type": "text"
      },
      "source": [
        "Let's see the heads of the training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db2rSoTZzZt-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "3d36316c-428d-4228-a954-312e2d331019"
      },
      "source": [
        "california_train.head()"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65500.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -114.31     34.19  ...         1.4936             66900.0\n",
              "1    -114.47     34.40  ...         1.8200             80100.0\n",
              "2    -114.56     33.69  ...         1.6509             85700.0\n",
              "3    -114.57     33.64  ...         3.1917             73400.0\n",
              "4    -114.57     33.57  ...         1.9250             65500.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD3Vn0UC0Dkw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "8472727d-9700-4e35-b0eb-c8bfa75e365d"
      },
      "source": [
        "california_test.head()"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.05</td>\n",
              "      <td>37.37</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3885.0</td>\n",
              "      <td>661.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>606.0</td>\n",
              "      <td>6.6085</td>\n",
              "      <td>344700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.30</td>\n",
              "      <td>34.26</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1510.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>3.5990</td>\n",
              "      <td>176500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.81</td>\n",
              "      <td>33.78</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3589.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>5.7934</td>\n",
              "      <td>270500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.36</td>\n",
              "      <td>33.82</td>\n",
              "      <td>28.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.1359</td>\n",
              "      <td>330000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.67</td>\n",
              "      <td>36.33</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>2.9375</td>\n",
              "      <td>81700.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -122.05     37.37  ...         6.6085            344700.0\n",
              "1    -118.30     34.26  ...         3.5990            176500.0\n",
              "2    -117.81     33.78  ...         5.7934            270500.0\n",
              "3    -118.36     33.82  ...         6.1359            330000.0\n",
              "4    -119.67     36.33  ...         2.9375             81700.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h86tPbWpi214",
        "colab_type": "text"
      },
      "source": [
        "Let's inspect the training and testing data to ensure no missing values and each feature has the right data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybrzWKE7u9Fq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "04a2c795-7716-43a6-ca50-9c1e3031c55e"
      },
      "source": [
        "california_train.isna().sum()"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "longitude             0\n",
              "latitude              0\n",
              "housing_median_age    0\n",
              "total_rooms           0\n",
              "total_bedrooms        0\n",
              "population            0\n",
              "households            0\n",
              "median_income         0\n",
              "median_house_value    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr4wx5qUil3o",
        "colab_type": "text"
      },
      "source": [
        "No missing values in the training data set, let's confirm it has the right data types per feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ygPwAHgjcLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "2a882bef-31af-44dd-ba1b-9965abe08761"
      },
      "source": [
        "california_train.info()"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17000 entries, 0 to 16999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           17000 non-null  float64\n",
            " 1   latitude            17000 non-null  float64\n",
            " 2   housing_median_age  17000 non-null  float64\n",
            " 3   total_rooms         17000 non-null  float64\n",
            " 4   total_bedrooms      17000 non-null  float64\n",
            " 5   population          17000 non-null  float64\n",
            " 6   households          17000 non-null  float64\n",
            " 7   median_income       17000 non-null  float64\n",
            " 8   median_house_value  17000 non-null  float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 1.2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDVQG2wpjnXc",
        "colab_type": "text"
      },
      "source": [
        "Asesome! all data types have the right values. Let's do so for the Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf-9CdqqjuPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "4a355df7-200a-4067-abab-aa7bd5a44755"
      },
      "source": [
        "california_test.isna().sum()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "longitude             0\n",
              "latitude              0\n",
              "housing_median_age    0\n",
              "total_rooms           0\n",
              "total_bedrooms        0\n",
              "population            0\n",
              "households            0\n",
              "median_income         0\n",
              "median_house_value    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdpF9v8_j1P9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "48bffa66-73ef-4e47-ed86-f1469559bfe0"
      },
      "source": [
        "california_test.info()"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           3000 non-null   float64\n",
            " 1   latitude            3000 non-null   float64\n",
            " 2   housing_median_age  3000 non-null   float64\n",
            " 3   total_rooms         3000 non-null   float64\n",
            " 4   total_bedrooms      3000 non-null   float64\n",
            " 5   population          3000 non-null   float64\n",
            " 6   households          3000 non-null   float64\n",
            " 7   median_income       3000 non-null   float64\n",
            " 8   median_house_value  3000 non-null   float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 211.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9r8lSqDS4Mt",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>2.1: Splitting and Re-shaping the data:</b></h4> \n",
        "\n",
        "So let's split the training and testing sets into sub train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3owRvIpLTGvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7fc7ff42-20b7-438e-d986-02dde01e38c4"
      },
      "source": [
        "# First let's make copies of the training and testing sets as numpy arrays\n",
        "train_arr = california_train.values\n",
        "test_arr = california_test.values\n",
        "\n",
        "# Next, let's create the features and labels for both training and testing sets.\n",
        "x_train, y_train = train_arr[:,:-1], train_arr[:,-1]\n",
        "x_test, y_test = test_arr[:,:-1], test_arr[:,-1]\n",
        "\n",
        "# Let's print the shapes of the training and testing labels\n",
        "print(f'x_train shape is:- {x_train.shape} and y_train shape is {y_train.shape}.')\n",
        "print(f'x_test shape is:- {x_test.shape} and y_test shape is {y_test.shape}.')"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape is:- (17000, 8) and y_train shape is (17000,).\n",
            "x_test shape is:- (3000, 8) and y_test shape is (3000,).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFEkCwhBZRkT",
        "colab_type": "text"
      },
      "source": [
        "Next, let's reshape the training and testing sets to become a transpose of the current shape, but making sure we don't have rank-1 arrays in the process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmSaGA-HT41l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "170be8b5-5c9b-426a-f9b6-a6a0b85d4f72"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], -1).T\n",
        "y_train = y_train.reshape(y_train.shape[0], -1).T\n",
        "x_test = x_test.reshape(x_test.shape[0], -1).T\n",
        "y_test = y_test.reshape(y_test.shape[0], -1).T\n",
        "\n",
        "# Let's print out the shapes again\n",
        "print(f'x_train shape is:- {x_train.shape} and y_train shape is {y_train.shape}.')\n",
        "print(f'x_test shape is:- {x_test.shape} and y_test shape is {y_test.shape}.')"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape is:- (8, 17000) and y_train shape is (1, 17000).\n",
            "x_test shape is:- (8, 3000) and y_test shape is (1, 3000).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XC37HNiagH6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<h4><b>2.2: Feature Normalization:</b></h4>\n",
        "\n",
        "Let's normalize the training sets. Let's use the Z-Score or standard score normalization. Let's define a Z_score method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5f29Q5Da29k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Z_score(x):\n",
        "    \"\"\"Compute z_score of a distribution.\n",
        "\n",
        "    @param:\n",
        "    x is an array or dataframe of ints or floats\n",
        "\n",
        "    @Return:\n",
        "    Returns z_score normalisation applied to x\n",
        "    \"\"\"\n",
        "    mean = np.mean(x)\n",
        "    std = np.std(x)\n",
        "    zee_score = (x - mean) / std\n",
        "    \n",
        "    return zee_score"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oPZlBw_m5Sr",
        "colab_type": "text"
      },
      "source": [
        "Now let's apply the z_score normalisation to the training sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDdUxS6JfP9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bef5e106-a1b9-4e31-d07d-93c53228e172"
      },
      "source": [
        "x_train_norm = np.apply_along_axis(Z_score, 1, x_train)\n",
        "x_test_norm = np.apply_along_axis(Z_score, 1, x_test)\n",
        "\n",
        "# Let's confirm they still have the same shape\n",
        "print(x_train_norm.shape == x_train.shape)\n",
        "print(x_test_norm.shape == x_test.shape)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLx3bpEKoLzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "a1ebbb1d-9077-4211-cfa4-8a93a29eaf26"
      },
      "source": [
        "# Let's see the first few elements of the x_train_norm array\n",
        "x_train_norm[:5]"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.619365  ,  2.53956878,  2.4946834 , ..., -2.36291168,\n",
              "        -2.36291168, -2.387848  ],\n",
              "       [-0.67152023, -0.57326437, -0.90546278, ...,  2.90780067,\n",
              "         2.88908527,  2.29955006],\n",
              "       [-1.07967114, -0.76187201, -0.92077158, ..., -0.92077158,\n",
              "        -0.76187201,  1.85997083],\n",
              "       [ 1.36169494,  2.29660752, -0.88246225, ...,  0.01529238,\n",
              "         0.01299867, -0.377848  ],\n",
              "       [ 1.76420407,  3.23044127, -0.86695622, ..., -0.01995512,\n",
              "         0.02986848, -0.56801465]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u7M5mT0phOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NKzfNoWkXS8",
        "colab_type": "text"
      },
      "source": [
        "## **PART 3: Intro to Building a Neural Network From Scratch:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmxtlEogpyOI",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>Building a Logistic Regression model as a Neural Network</b></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgfjQa_wRZFe",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression with a Neural Network mindset:**\n",
        "\n",
        "I will build a logistic regression classifier as a Neural Network to predict housing prices\n",
        "\n",
        "Steps Include:\n",
        "\n",
        "1. Do not use loops (for/while) unless absolutely necessary\n",
        "2. Build the general architecture of a learning algorithm, including:\n",
        "Initializing parameters\n",
        "3. Calculate the cost function and its gradient\n",
        "4. Use an optimization algorithm (gradient descent)\n",
        "5. Gather all three functions above into a main model function, in the right order.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQbnrsbsrnvw",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>Mathematical expression of the algorithm:</b></h4>\n",
        "\n",
        "For one example $x^{(i)}$:$$z^{(i)} = w^T x^{(i)} + b $$$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
        "\n",
        "The cost is then computed by summing over all training examples:$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n",
        "\n",
        "Key steps: In this exercise, we will carry out the following steps:\n",
        "\n",
        "- Initialize the parameters of the model\n",
        "- Learn the parameters for the model by minimizing the cost  \n",
        "- Use the learned parameters to make predictions (on the test set)\n",
        "- Analyse the results and conclude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHSabxdgLWaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-yU0DxA4Vw0",
        "colab_type": "text"
      },
      "source": [
        "<h2><b>Part 4: Building the parts of our algorithm</b></h2>\n",
        "\n",
        "The main steps for building a Neural Network are:\n",
        "\n",
        "1. Define the model structure (such as number of input features)\n",
        "2. Initialize the model's parameters\n",
        "3. Loop:\n",
        ">>1. Calculate current loss (forward propagation)\n",
        ">>2. Calculate current gradient (backward propagation)\n",
        ">>3. Update parameters (gradient descent)\n",
        "\n",
        "I will build 1-3 separately and integrate them into one function called a  model()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvQxb0I45lsA",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>4.1: Helper Function:</b></h4> \n",
        "\n",
        "Let's create the sigmoid function we shall apply to the linear function of each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPMiPLamkf2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "\n",
        "    s = np.divide(1, 1 + np.exp(-z))\n",
        "\n",
        "    return s"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN2UqufM7fM4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef76588d-b2c9-4c55-e784-7a787ae35eca"
      },
      "source": [
        "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid([0, 2]) = [0.5        0.88079708]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxxMq6y96_lQ",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>4.2: Initializing Parameters:</b></h4> \n",
        "\n",
        "We initialise the weights and bias parameters. The weights should take the shape of `(num_features, 1)`, while bias should be initialised to `0`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QklvH4bl9iMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_params(x):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (x.shape[0], 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    x -- an array of features\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape np.zeros((x.shape[0], 1))\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    dim = x.shape[0]\n",
        "\n",
        "    w = np.zeros((dim,1)) * 0.01\n",
        "    \n",
        "    b = 0\n",
        "\n",
        "    # Let's run some assertions on the shape of w and type of b.\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYrxQgdP_s2l",
        "colab_type": "text"
      },
      "source": [
        "Let's test the initialise params function.\n",
        "We shall create a random array of shape (5, 3), then apply the initialise_params function to it. We should get w of zeros of shape (5,1) and b of 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgmXERcp-rDw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "1a310e33-8260-4359-c16b-dfeaf08697c8"
      },
      "source": [
        "t = np.random.rand(5,3)\n",
        "t"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.72025925, 0.75259474, 0.22328088],\n",
              "       [0.24397651, 0.17971476, 0.80619987],\n",
              "       [0.45574931, 0.07405302, 0.18415469],\n",
              "       [0.58082531, 0.96836695, 0.04981755],\n",
              "       [0.54359109, 0.84930552, 0.7502284 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrZidNKV_fs8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "fefbd643-eaeb-4129-d347-20db88d0876e"
      },
      "source": [
        "w = initialise_params(t)[0]\n",
        "b = initialise_params(t)[1]\n",
        "\n",
        "# Let's see w and b\n",
        "print(f'w =\\n{w}\\n\\nb =\\n{b}')"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w =\n",
            "[[ 0.00281479]\n",
            " [-0.00380917]\n",
            " [ 0.01373918]\n",
            " [-0.01519343]\n",
            " [-0.01109288]]\n",
            "\n",
            "b =\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg5hXKPHDH6A",
        "colab_type": "text"
      },
      "source": [
        "Let's confirm that t.shape[0] == w.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH3QF3zG_jDv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a214adb5-3d44-4af4-e89e-94f88ba2fac7"
      },
      "source": [
        "t.shape[0] == w.shape[0]"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULkZPQzvB1J3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3348f71d-b85a-43d5-8d28-0e0e97ed5c2d"
      },
      "source": [
        "assert w.shape == (5,1)\n",
        "print('Yes! w.shape == (5,1)')"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yes! w.shape == (5,1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu2Gz4k8D3Vw",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>4.3: Forward and Backward Propagation:</b></h4> \n",
        "\n",
        "Now that the parameters are initialized, I can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
        "\n",
        "I need to Implement a function propagate() that computes the cost function and its gradient.\n",
        "\n",
        "Cues:\n",
        "\n",
        "**Forward Propagation:**\n",
        "\n",
        "* I get X\n",
        "* I compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
        "* I calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$<br>\n",
        "Here are the two formulas I will be using:\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv53ayeYD92P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_features, 1)\n",
        "    b -- bias, a scalar \n",
        "    X -- data of size (num_features, num_examples)\n",
        "    Y -- true \"label\" vector (containing the true values of the houses) of size (1, num_examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    Tips:\n",
        "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # Forward-prop from X to cost..\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    \n",
        "    # Now we compute the cost\n",
        "    cost = np.multiply(-(1/m), np.sum(Y * np.log(A) + ((1 - Y) * np.log(1 - A))))\n",
        "\n",
        "    # Backward-prop (to find grads)\n",
        "    dw = np.multiply(1/m, np.dot(X, (A - Y).T))\n",
        "    db = np.multiply(1/m, np.sum(A - Y))\n",
        "\n",
        "    # Let's write some shape assertions\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "\n",
        "    grads = {'dw':dw, 'db':db}\n",
        "\n",
        "    return grads, cost"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xzEBAETlPN4",
        "colab_type": "text"
      },
      "source": [
        "**Testing:** Let's test the propagate function above with some values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UeS-8wyhTbm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "3edb0af6-e29d-4a1c-b54c-6445f354dd8a"
      },
      "source": [
        "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dw = [[0.99845601]\n",
            " [2.39507239]]\n",
            "db = 0.001455578136784208\n",
            "cost = 5.801545319394553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFP0m86Gh3_l",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>4.4 - Optimization:</h4></b>\n",
        "\n",
        "So, I have initialized the parameters.\n",
        "I have computed a cost function and its gradient.\n",
        "Now I need to update the parameters using gradient descent.\n",
        "\n",
        "\n",
        "The goal is to learn $w$ and $b$ by minimizing the cost function $J$. Therefore... <br>For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IROH2McwkeLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_features, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_features, num_examples)\n",
        "    Y -- true \"label\" vector (containing true values of the houses), of shape (1, num_examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the optimized weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    Tips:\n",
        "    I basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        grads, cost = propagate(w,b,X,Y)\n",
        "        # retrieve derivatives from grads\n",
        "        dw = grads['dw']\n",
        "        db = grads['db']\n",
        "\n",
        "        # update w and b based on derivatives\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        # Now let's record the costs per 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVmXC3GosepS",
        "colab_type": "text"
      },
      "source": [
        "**Testing:** Using the same values from the last test, let's test the optimize function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-6iZ3eVq6en",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "0bcaac2b-ad22-4cb4-9e27-0e6894118ef6"
      },
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[0.19033591]\n",
            " [0.12259159]]\n",
            "b = 1.9253598300845747\n",
            "dw = [[0.67752042]\n",
            " [1.41625495]]\n",
            "db = 0.21919450454067657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzlhnU6CtXiz",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** The previous function computes the learned $w$ and $b$ values. Therefore I can use these to predict the labels for a dataset X. in fact, let me define a predict() function. This basically takes one step:\n",
        "\n",
        "Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j61qHY62tcI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict the label for a given data set using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_features , 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_features , num_examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities of the true label Y\n",
        "    A = sigmoid((np.dot(w.T,X)+b))\n",
        "    \n",
        "    # Let's write some assert statements\n",
        "    assert(A.shape == (1, m))\n",
        "    \n",
        "    return A"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XncA4SXqwGUw",
        "colab_type": "text"
      },
      "source": [
        "**Testing:** Testing the predict function above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAv_L4S1vkIx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "134eb65e-2807-414b-e2f4-225337d157a4"
      },
      "source": [
        "w = np.array([[0.1124579],[0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
        "print (\"predictions = \" + str(predict(w, b, X)))"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions = [[0.52241976 0.50960677 0.34597965]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdhwWQ3EwVvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO6MRKmYwWXu",
        "colab_type": "text"
      },
      "source": [
        "<h2><b>Part 5: Merge all functions into a model:</b></h2>\n",
        "\n",
        "Putting it all together... All the building blocks (functions implemented in the previous parts) together, in the right order.\n",
        "\n",
        "I will Implement the model function. Using the following notations:\n",
        "\n",
        "- y_prediction_test for my predictions on the test set\n",
        "- y_prediction_train for my predictions on the train set\n",
        "- w, costs, grads for the outputs of optimize()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih3LoePQwcOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the functions implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_features, training_examples)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, training_examples)\n",
        "    X_test -- test set represented by a numpy array of shape (num_features, testing_examples)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, testing_examples)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize parameters with zeros\n",
        "    w, b = np.zeros((X_train.shape[0], 1)), 0\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eejBkMOOzcRj",
        "colab_type": "text"
      },
      "source": [
        "<h3><b>Predictions and Model Evaluation</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhTVBP6czYWi",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's predict the housing prices using the training and testing data sets we normalised earlier. Passing these to the model we just assembled above. But first, let's re-confirm the shapes of these data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwvmF_3K0C6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a43eb2f0-453b-490b-fd42-2c28b89467e0"
      },
      "source": [
        "print(f'x_train_norm shape is {x_train_norm.shape}, y_train shape is {y_train.shape}')\n",
        "print(f'x_test_norm shape is {x_test_norm.shape}, y_test shape is {y_test.shape}')"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_norm shape is (8, 17000), y_train shape is (1, 17000)\n",
            "x_test_norm shape is (8, 3000), y_test shape is (1, 3000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkhqwTYGzEla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2ef92943-392c-4ae9-a655-f2172f754a6a"
      },
      "source": [
        "d = model(x_train_norm, y_train, x_test_norm, y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train accuracy: -20729891.23529412 %\n",
            "test accuracy: -20584427.5 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYF2NDKE2oij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "28970a38-955a-41fe-d33b-facbaede4c8d"
      },
      "source": [
        "# Plot learning curve (with costs)\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlbrY3qC2v_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "3346c6eb-6978-47c8-9ac3-1df1d58e04b4"
      },
      "source": [
        "d['costs']"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6931471805592541,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf,\n",
              " -inf]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4911R7v7X0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "235e344d-a0e3-4837-b1d9-ba1fb780d8e5"
      },
      "source": [
        "x_train_norm"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.619365  ,  2.53956878,  2.4946834 , ..., -2.36291168,\n",
              "        -2.36291168, -2.387848  ],\n",
              "       [-0.67152023, -0.57326437, -0.90546278, ...,  2.90780067,\n",
              "         2.88908527,  2.29955006],\n",
              "       [-1.07967114, -0.76187201, -0.92077158, ..., -0.92077158,\n",
              "        -0.76187201,  1.85997083],\n",
              "       ...,\n",
              "       [-0.36118401, -0.26186523, -0.95535424, ..., -0.16167524,\n",
              "        -0.1146295 , -0.54326844],\n",
              "       [-0.07599796, -0.09940441, -0.99925206, ..., -0.11760942,\n",
              "        -0.06039367, -0.60134255],\n",
              "       [-1.25254316, -1.08148298, -1.17010515, ..., -0.44666313,\n",
              "        -0.99778717, -0.45536288]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1IyvrU97dcV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "14c93cb9-93fb-4787-f931-7f36342ff7fc"
      },
      "source": [
        "x_test_norm"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.23372874,  0.64634391,  0.89200673, ..., -0.05554988,\n",
              "         1.2379401 , -0.02045519],\n",
              "       [ 0.81463305, -0.64593087, -0.87135553, ...,  0.31212392,\n",
              "        -0.72107242, -0.57078932],\n",
              "       [-0.14699983,  1.12756514, -0.14699983, ..., -1.5012251 ,\n",
              "         0.88858421,  1.04790483],\n",
              "       ...,\n",
              "       [ 0.13024561, -0.57629585,  0.07880784, ..., -0.68887663,\n",
              "        -1.31680565, -0.63064519],\n",
              "       [ 0.31773437, -0.58274291,  0.01392592, ..., -0.73875265,\n",
              "        -1.30257733, -0.62927213],\n",
              "       [ 1.51074547, -0.11232419,  1.0711495 , ..., -0.81855768,\n",
              "        -0.2893275 ,  2.56365089]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HciyGZzG7gTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aea3ef9a-a6f7-4f5c-9625-50411a8a368a"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 66900.,  80100.,  85700., ..., 103600.,  85800.,  94600.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa2AYyqY7jR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "699e9c89-ba82-4308-ddfa-93dedbdda79c"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[344700., 176500., 270500., ...,  62000., 162500., 500001.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    }
  ]
}