{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part_1_predicting_california_housing_prices.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqf35Pj4pYwxEZzrdUkF9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lawrence-Krukrubo/Predicting_California_Housing_Prices/blob/master/part_1_predicting_california_housing_prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keq1uqwC6CS4",
        "colab_type": "text"
      },
      "source": [
        "We shall hard-code a Neural Network using python and use it to predict the price of houses in california. <br> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7o1dvcp88Xg",
        "colab_type": "text"
      },
      "source": [
        "The [Universal Function Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that a neural network with a single hidden layer and a finite number of neurons can approximate continous functions on compact subsets of data points in hyper-dimensional vector spaces, under mild assumptions of the activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgSV_gAkj_pF",
        "colab_type": "text"
      },
      "source": [
        "## **PART 1: The Needed Modules:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-_XA1zyxTTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2aea9e9-654e-4e42-fbb2-dbdabedce81a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "import sklearn.linear_model\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "print('all modules imported!')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all modules imported!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2LRj5F1KpV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsAnPZGKkL7_",
        "colab_type": "text"
      },
      "source": [
        "## **PART 2: The Data and Preprocessing:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfBj1-Jox4N0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0f6a6511-1794-4214-87ab-30c0c33fe10f"
      },
      "source": [
        "california_train = pd.read_csv('sample_data/california_housing_train.csv')\n",
        "california_test = pd.read_csv('sample_data/california_housing_test.csv')\n",
        "print(f'Shape of training data is: {california_train.shape},\\nShape of testing data is: {california_test.shape}')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training data is: (17000, 9),\n",
            "Shape of testing data is: (3000, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XOrxMDxJpGz",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>The Data Dictionary:</b></h4>\n",
        "\n",
        "1. **longitude:** <br>A measure of how far west a house is; a higher value is farther west\n",
        "\n",
        "2. **latitude:** <br>A measure of how far north a house is; a higher value is farther north\n",
        "\n",
        "3. **housingMedianAge:** <br>Median age of a house within a block; a lower number is a newer building\n",
        "\n",
        "4. **totalRooms:** <br>Total number of rooms within a block\n",
        "\n",
        "5. **totalBedrooms:** <br>Total number of bedrooms within a block\n",
        "\n",
        "6. **population:** <br>Total number of people residing within a block\n",
        "\n",
        "7. **households:** <br>Total number of households, a group of people residing within a home unit, for a block\n",
        "\n",
        "8. **medianIncome:** <br>Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
        "\n",
        "9. **medianHouseValue:** <br>Median house value for households within a block (measured in US Dollars)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzuy-eADKgjo",
        "colab_type": "text"
      },
      "source": [
        "Let's see the heads of the training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db2rSoTZzZt-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "bdf864a2-bd7a-40b6-a254-3fc39dc2e001"
      },
      "source": [
        "california_train.head()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65500.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -114.31     34.19  ...         1.4936             66900.0\n",
              "1    -114.47     34.40  ...         1.8200             80100.0\n",
              "2    -114.56     33.69  ...         1.6509             85700.0\n",
              "3    -114.57     33.64  ...         3.1917             73400.0\n",
              "4    -114.57     33.57  ...         1.9250             65500.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD3Vn0UC0Dkw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "fd5c58eb-2824-4302-e0f3-d33c0353e517"
      },
      "source": [
        "california_test.head()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.05</td>\n",
              "      <td>37.37</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3885.0</td>\n",
              "      <td>661.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>606.0</td>\n",
              "      <td>6.6085</td>\n",
              "      <td>344700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.30</td>\n",
              "      <td>34.26</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1510.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>3.5990</td>\n",
              "      <td>176500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.81</td>\n",
              "      <td>33.78</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3589.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>5.7934</td>\n",
              "      <td>270500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.36</td>\n",
              "      <td>33.82</td>\n",
              "      <td>28.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.1359</td>\n",
              "      <td>330000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.67</td>\n",
              "      <td>36.33</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>2.9375</td>\n",
              "      <td>81700.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -122.05     37.37  ...         6.6085            344700.0\n",
              "1    -118.30     34.26  ...         3.5990            176500.0\n",
              "2    -117.81     33.78  ...         5.7934            270500.0\n",
              "3    -118.36     33.82  ...         6.1359            330000.0\n",
              "4    -119.67     36.33  ...         2.9375             81700.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h86tPbWpi214",
        "colab_type": "text"
      },
      "source": [
        "Let's inspect the training and testing data to ensure no missing values and each feature has the right data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybrzWKE7u9Fq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "496bc1ae-d855-4279-c9c0-5c28bfb7c6db"
      },
      "source": [
        "california_train.isna().sum()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "longitude             0\n",
              "latitude              0\n",
              "housing_median_age    0\n",
              "total_rooms           0\n",
              "total_bedrooms        0\n",
              "population            0\n",
              "households            0\n",
              "median_income         0\n",
              "median_house_value    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr4wx5qUil3o",
        "colab_type": "text"
      },
      "source": [
        "No missing values in the training data set, let's confirm it has the right data types per feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ygPwAHgjcLJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "6616ae94-6a40-435c-ddb4-89d35933cbb9"
      },
      "source": [
        "california_train.info()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17000 entries, 0 to 16999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           17000 non-null  float64\n",
            " 1   latitude            17000 non-null  float64\n",
            " 2   housing_median_age  17000 non-null  float64\n",
            " 3   total_rooms         17000 non-null  float64\n",
            " 4   total_bedrooms      17000 non-null  float64\n",
            " 5   population          17000 non-null  float64\n",
            " 6   households          17000 non-null  float64\n",
            " 7   median_income       17000 non-null  float64\n",
            " 8   median_house_value  17000 non-null  float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 1.2 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDVQG2wpjnXc",
        "colab_type": "text"
      },
      "source": [
        "Asesome! all data types have the right values. Let's do so for the Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf-9CdqqjuPQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "242c9071-c84c-46eb-8e8e-c6aec6920c81"
      },
      "source": [
        "california_test.isna().sum()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "longitude             0\n",
              "latitude              0\n",
              "housing_median_age    0\n",
              "total_rooms           0\n",
              "total_bedrooms        0\n",
              "population            0\n",
              "households            0\n",
              "median_income         0\n",
              "median_house_value    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdpF9v8_j1P9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "69988804-783c-42f8-fd1e-464b0a200b00"
      },
      "source": [
        "california_test.info()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           3000 non-null   float64\n",
            " 1   latitude            3000 non-null   float64\n",
            " 2   housing_median_age  3000 non-null   float64\n",
            " 3   total_rooms         3000 non-null   float64\n",
            " 4   total_bedrooms      3000 non-null   float64\n",
            " 5   population          3000 non-null   float64\n",
            " 6   households          3000 non-null   float64\n",
            " 7   median_income       3000 non-null   float64\n",
            " 8   median_house_value  3000 non-null   float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 211.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9r8lSqDS4Mt",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>2.1: Splitting and Re-shaping the data:</b></h4> \n",
        "\n",
        "So let's split the training and testing sets into sub train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3owRvIpLTGvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "35e662ee-9d1c-4595-ff15-6b67bc60835b"
      },
      "source": [
        "# First let's make copies of the training and testing sets as numpy arrays\n",
        "train_arr = california_train.values\n",
        "test_arr = california_test.values\n",
        "\n",
        "# Next, let's create the features and labels for both training and testing sets.\n",
        "x_train, y_train = train_arr[:,:-1], train_arr[:,-1]\n",
        "x_test, y_test = test_arr[:,:-1], test_arr[:,-1]\n",
        "\n",
        "# Let's print the shapes of the training and testing labels\n",
        "print(f'x_train shape is:- {x_train.shape} and y_train shape is {y_train.shape}.')\n",
        "print(f'x_test shape is:- {x_test.shape} and y_test shape is {y_test.shape}.')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape is:- (17000, 8) and y_train shape is (17000,).\n",
            "x_test shape is:- (3000, 8) and y_test shape is (3000,).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFEkCwhBZRkT",
        "colab_type": "text"
      },
      "source": [
        "Next, let's reshape the training and testing sets to become a transpose of the current shape, but making sure we don't have rank-1 arrays in the process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmSaGA-HT41l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b3e77f23-a03a-4240-e944-3564c40d4eb2"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], -1).T\n",
        "y_train = y_train.reshape(y_train.shape[0], -1).T\n",
        "x_test = x_test.reshape(x_test.shape[0], -1).T\n",
        "y_test = y_test.reshape(y_test.shape[0], -1).T\n",
        "\n",
        "# Let's print out the shapes again\n",
        "print(f'x_train shape is:- {x_train.shape} and y_train shape is {y_train.shape}.')\n",
        "print(f'x_test shape is:- {x_test.shape} and y_test shape is {y_test.shape}.')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape is:- (8, 17000) and y_train shape is (1, 17000).\n",
            "x_test shape is:- (8, 3000) and y_test shape is (1, 3000).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XC37HNiagH6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "<h4><b>2.2: Feature Normalization:</b></h4>\n",
        "\n",
        "Let's normalize the training sets. Let's use the Z-Score or standard score normalization. Let's define a Z_score method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5f29Q5Da29k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Z_score(x):\n",
        "    \"\"\"Compute z_score of a distribution.\n",
        "\n",
        "    @param:\n",
        "    x is an array or dataframe of ints or floats\n",
        "\n",
        "    @Return:\n",
        "    Returns z_score normalisation applied to x\n",
        "    \"\"\"\n",
        "    mean = np.mean(x)\n",
        "    std = np.std(x)\n",
        "    zee_score = (x - mean) / std\n",
        "    \n",
        "    return zee_score"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oPZlBw_m5Sr",
        "colab_type": "text"
      },
      "source": [
        "Now let's apply the z_score normalisation to the training sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDdUxS6JfP9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c1fc7655-9933-4d3a-f988-d90f1d7e1762"
      },
      "source": [
        "x_train_norm = np.apply_along_axis(Z_score, 1, x_train)\n",
        "x_test_norm = np.apply_along_axis(Z_score, 1, x_test)\n",
        "\n",
        "# Let's confirm they still have the same shape\n",
        "print(x_train_norm.shape == x_train.shape)\n",
        "print(x_test_norm.shape == x_test.shape)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLx3bpEKoLzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "a24d9afc-8491-49e6-c784-78d12ef08998"
      },
      "source": [
        "# Let's see the first few elements of the x_train_norm array\n",
        "x_train_norm[:5]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.619365  ,  2.53956878,  2.4946834 , ..., -2.36291168,\n",
              "        -2.36291168, -2.387848  ],\n",
              "       [-0.67152023, -0.57326437, -0.90546278, ...,  2.90780067,\n",
              "         2.88908527,  2.29955006],\n",
              "       [-1.07967114, -0.76187201, -0.92077158, ..., -0.92077158,\n",
              "        -0.76187201,  1.85997083],\n",
              "       [ 1.36169494,  2.29660752, -0.88246225, ...,  0.01529238,\n",
              "         0.01299867, -0.377848  ],\n",
              "       [ 1.76420407,  3.23044127, -0.86695622, ..., -0.01995512,\n",
              "         0.02986848, -0.56801465]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u7M5mT0phOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NKzfNoWkXS8",
        "colab_type": "text"
      },
      "source": [
        "## **PART 3: Intro to Building a Neural Network From Scratch:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmxtlEogpyOI",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>Building a Logistic Regression model as a Neural Network</b></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgfjQa_wRZFe",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression with a Neural Network mindset:**\n",
        "\n",
        "I will build a logistic regression classifier as a Neural Network to predict housing prices\n",
        "\n",
        "Steps Include:\n",
        "\n",
        "1. Do not use loops (for/while) unless absolutely necessary\n",
        "2. Build the general architecture of a learning algorithm, including:\n",
        "Initializing parameters\n",
        "3. Calculate the cost function and its gradient\n",
        "4. Use an optimization algorithm (gradient descent)\n",
        "5. Gather all three functions above into a main model function, in the right order.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQbnrsbsrnvw",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>Mathematical expression of the algorithm:</b></h4>\n",
        "\n",
        "For one example $x^{(i)}$:$$z^{(i)} = w^T x^{(i)} + b $$$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
        "\n",
        "The cost is then computed by summing over all training examples:$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n",
        "\n",
        "Key steps: In this exercise, we will carry out the following steps:\n",
        "\n",
        "- Initialize the parameters of the model\n",
        "- Learn the parameters for the model by minimizing the cost  \n",
        "- Use the learned parameters to make predictions (on the test set)\n",
        "- Analyse the results and conclude"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHSabxdgLWaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-yU0DxA4Vw0",
        "colab_type": "text"
      },
      "source": [
        "<h2><b>Part 4: Building the parts of our algorithm</b></h2>\n",
        "\n",
        "The main steps for building a Neural Network are:\n",
        "\n",
        "1. Define the model structure (such as number of input features)\n",
        "2. Initialize the model's parameters\n",
        "3. Loop:\n",
        ">>1. Calculate current loss (forward propagation)\n",
        ">>2. Calculate current gradient (backward propagation)\n",
        ">>3. Update parameters (gradient descent)\n",
        "\n",
        "I will build 1-3 separately and integrate them into one function called a  model()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxxMq6y96_lQ",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>4.1: Initializing Parameters:</b></h4> \n",
        "\n",
        "We initialise the weights and bias parameters. The weights should take the shape of `(num_features, 1)`, while bias should be initialised to `0`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QklvH4bl9iMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_params(x):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (x.shape[0], 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    x -- an array of features\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape np.zeros((x.shape[0], 1))\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    dim = x.shape[0]\n",
        "\n",
        "    w = np.zeros((dim,1)) * 0.01\n",
        "    \n",
        "    b = 0\n",
        "\n",
        "    # Let's run some assertions on the shape of w and type of b.\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYrxQgdP_s2l",
        "colab_type": "text"
      },
      "source": [
        "Let's test the initialise params function.\n",
        "We shall create a random array of shape (5, 3), then apply the initialise_params function to it. We should get w of zeros of shape (5,1) and b of 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgmXERcp-rDw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "84bfb2e0-2973-4f56-e62e-43cbe5453f41"
      },
      "source": [
        "t = np.random.rand(5,3)\n",
        "t"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.41423502, 0.29607993, 0.62878791],\n",
              "       [0.57983781, 0.5999292 , 0.26581912],\n",
              "       [0.28468588, 0.25358821, 0.32756395],\n",
              "       [0.1441643 , 0.16561286, 0.96393053],\n",
              "       [0.96022672, 0.18841466, 0.02430656]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrZidNKV_fs8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "40bcd809-c0e0-4432-8059-781154903e0c"
      },
      "source": [
        "w = initialise_params(t)[0]\n",
        "b = initialise_params(t)[1]\n",
        "\n",
        "# Let's see w and b\n",
        "print(f'w =\\n{w}\\n\\nb =\\n{b}')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w =\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "\n",
            "b =\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg5hXKPHDH6A",
        "colab_type": "text"
      },
      "source": [
        "Let's confirm that t.shape[0] == w.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH3QF3zG_jDv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2a3be26-36b1-40cf-d1ed-ff622d0b5926"
      },
      "source": [
        "t.shape[0] == w.shape[0]"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULkZPQzvB1J3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2e413cb-0a4d-44f5-c442-4decd0281c7c"
      },
      "source": [
        "assert w.shape == (5,1)\n",
        "print('Yes! w.shape == (5,1)')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yes! w.shape == (5,1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu2Gz4k8D3Vw",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>4.2: Forward and Backward Propagation:</b></h4> \n",
        "\n",
        "Now that the parameters are initialized, I can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
        "\n",
        "I need to Implement a function propagate() that computes the cost function and its gradient.\n",
        "\n",
        "Cues:\n",
        "\n",
        "**Forward Propagation:**\n",
        "\n",
        "* I get X\n",
        "* I compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
        "* I calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$<br>\n",
        "Here are the two formulas I will be using:\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv53ayeYD92P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_features, 1)\n",
        "    b -- bias, a scalar \n",
        "    X -- data of size (num_features, num_examples)\n",
        "    Y -- true \"label\" vector (containing the true values of the houses) of size (1, num_examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    Tips:\n",
        "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # Forward-prop from X to cost..\n",
        "    A = np.dot(w.T, X) + b\n",
        "    \n",
        "    # Now we compute the cost using the mean absolute error\n",
        "    cost = np.divide(np.sum(np.abs(A - Y)), m)\n",
        "\n",
        "\n",
        "    # Backward-prop (to find grads)\n",
        "    dw = np.multiply(1/m, np.dot(X, (A - Y).T))\n",
        "    db = np.multiply(1/m, np.sum(A - Y))\n",
        "\n",
        "    # Let's write some shape assertions\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "\n",
        "    grads = {'dw':dw, 'db':db}\n",
        "\n",
        "    return grads, cost"
      ],
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xzEBAETlPN4",
        "colab_type": "text"
      },
      "source": [
        "**Testing:** Let's test the propagate function above with some values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UeS-8wyhTbm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "e6fcd27d-ba5b-4ac6-d5fa-9a66debaa2b4"
      },
      "source": [
        "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dw = [[12.8       ]\n",
            " [30.82666667]]\n",
            "db = 4.533333333333333\n",
            "cost = 8.799999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFP0m86Gh3_l",
        "colab_type": "text"
      },
      "source": [
        "<h4><b>4.3 - Optimization:</h4></b>\n",
        "\n",
        "So, I have initialized the parameters.\n",
        "I have computed a cost function and its gradient.\n",
        "Now I need to update the parameters using gradient descent.\n",
        "\n",
        "\n",
        "The goal is to learn $w$ and $b$ by minimizing the cost function $J$. Therefore... <br>For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IROH2McwkeLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_features, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_features, num_examples)\n",
        "    Y -- true \"label\" vector (containing true values of the houses), of shape (1, num_examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the optimized weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    Tips:\n",
        "    I basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        grads, cost = propagate(w,b,X,Y)\n",
        "        # retrieve derivatives from grads\n",
        "        dw = grads['dw']\n",
        "        db = grads['db']\n",
        "\n",
        "        # update w and b based on derivatives\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        # Now let's record the costs per 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVmXC3GosepS",
        "colab_type": "text"
      },
      "source": [
        "**Testing:** Using the same values from the last test, let's test the optimize function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-6iZ3eVq6en",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "a3851e6f-d461-48b6-cac1-b4f7fc59736a"
      },
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[-0.04675219]\n",
            " [-0.12676061]]\n",
            "b = 1.223758731602527\n",
            "dw = [[ 0.12274692]\n",
            " [-0.09406359]]\n",
            "db = 0.36833971156600487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzlhnU6CtXiz",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** The previous function computes the learned $w$ and $b$ values. Therefore I can use these to predict the labels for a dataset X. in fact, let me define a predict() function. This basically takes one step:\n",
        "\n",
        "Calculate $\\hat{Y} = A = (w^T X + b)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j61qHY62tcI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict the label for a given data set using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_features , 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_features , num_examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities of the true label Y\n",
        "    A = np.dot(w.T,X)+b\n",
        "    \n",
        "    # Let's write some assert statements\n",
        "    assert(A.shape == (1, m))\n",
        "    \n",
        "    return A"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XncA4SXqwGUw",
        "colab_type": "text"
      },
      "source": [
        "**Testing:** Testing the predict function above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAv_L4S1vkIx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "253be78a-33bc-49ef-890d-f165c5610ca1"
      },
      "source": [
        "w = np.array([[0.1124579],[0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
        "print (\"predictions = \" + str(predict(w, b, X)))"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions = [[ 0.0897392   0.03843181 -0.6367585 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdhwWQ3EwVvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO6MRKmYwWXu",
        "colab_type": "text"
      },
      "source": [
        "<h2><b>Part 5: Merge all functions into a model:</b></h2>\n",
        "\n",
        "Putting it all together... All the building blocks (functions implemented in the previous parts) together, in the right order.\n",
        "\n",
        "I will Implement the model function. Using the following notations:\n",
        "\n",
        "- y_prediction_test for my predictions on the test set\n",
        "- y_prediction_train for my predictions on the train set\n",
        "- w, costs, grads for the outputs of optimize()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih3LoePQwcOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the functions implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_features, training_examples)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, training_examples)\n",
        "    X_test -- test set represented by a numpy array of shape (num_features, testing_examples)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, testing_examples)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize parameters with zeros\n",
        "    w, b = np.zeros((X_train.shape[0], 1)), 0\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eejBkMOOzcRj",
        "colab_type": "text"
      },
      "source": [
        "<h3><b>5.1 Predictions</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhTVBP6czYWi",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's predict the housing prices using the training and testing data sets we normalised earlier. Passing these to the model we just assembled above. But first, let's re-confirm the shapes of these data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwvmF_3K0C6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "38794891-ce3c-4a88-e3af-aec213841905"
      },
      "source": [
        "print(f'x_train_norm shape is {x_train_norm.shape}, y_train shape is {y_train.shape}')\n",
        "print(f'x_test_norm shape is {x_test_norm.shape}, y_test shape is {y_test.shape}')"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_norm shape is (8, 17000), y_train shape is (1, 17000)\n",
            "x_test_norm shape is (8, 3000), y_test shape is (1, 3000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkhqwTYGzEla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "f5ad5a1a-4690-40a4-d600-de067c3b22b3"
      },
      "source": [
        "d = model(x_train_norm, y_train, x_test_norm, y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 207300.912353\n",
            "Cost after iteration 100: 125821.848848\n",
            "Cost after iteration 200: 81771.408813\n",
            "Cost after iteration 300: 63648.663785\n",
            "Cost after iteration 400: 57563.991493\n",
            "Cost after iteration 500: 55753.955521\n",
            "Cost after iteration 600: 55214.119695\n",
            "Cost after iteration 700: 55019.088525\n",
            "Cost after iteration 800: 54863.793900\n",
            "Cost after iteration 900: 54680.911681\n",
            "Cost after iteration 1000: 54472.779383\n",
            "Cost after iteration 1100: 54247.953898\n",
            "Cost after iteration 1200: 54018.796698\n",
            "Cost after iteration 1300: 53792.923362\n",
            "Cost after iteration 1400: 53576.550766\n",
            "Cost after iteration 1500: 53371.085142\n",
            "Cost after iteration 1600: 53177.552701\n",
            "Cost after iteration 1700: 52996.553391\n",
            "Cost after iteration 1800: 52827.641276\n",
            "Cost after iteration 1900: 52671.857265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8WW7qBem6I",
        "colab_type": "text"
      },
      "source": [
        "<h3><b>5.2 Visualizing The Training Costs</b></h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYF2NDKE2oij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7e97d830-1233-4854-9497-7716cb6fa106"
      },
      "source": [
        "# Plot learning curve (with costs)\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yofVKMoW_rG",
        "colab_type": "text"
      },
      "source": [
        "<h3><b>5.3 Evaluations</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcJ__JnkXQuV",
        "colab_type": "text"
      },
      "source": [
        "I will evaluate the model's performance using the Mean Absolute Error (MAE), Mean-Squared-Error(MSE) and Root-Mean-Squared-Error(RMSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR2Dddk-XmaT",
        "colab_type": "text"
      },
      "source": [
        "**Mean Absolute Error (MAE):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7LKQtsnRvgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_absolute_error(y_true, y_pred):\n",
        "\n",
        "    abs_error = np.abs(y_true - y_pred)\n",
        "    sum_abs_error = np.sum(abs_error)\n",
        "    mae = sum_abs_error / y_true.shape[1]\n",
        "\n",
        "    return mae"
      ],
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMGpmPQIVt9I",
        "colab_type": "text"
      },
      "source": [
        "Let's see the Mean Absolute Errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUbOOX4EVOnq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebc41522-9f76-4e8a-fc5b-e5d75474387f"
      },
      "source": [
        "print(f'MAE for train set is {np.round(mean_absolute_error(y_train, d[\"Y_prediction_train\"]),2)} and MAE for test set is {np.round(mean_absolute_error(y_test, d[\"Y_prediction_test\"]),2)}')"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE for train set is 52526.75 and MAE for test set is 52255.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XF7qS6eXtyz",
        "colab_type": "text"
      },
      "source": [
        "**2. Mean Squared Error (MSE):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvdZX995Xzf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "\n",
        "    squared_error = (y_true - y_pred)**2\n",
        "    sum_squared_error = np.sum(squared_error)\n",
        "    mse = sum_squared_error / y_true.shape[1]\n",
        "\n",
        "    return mse"
      ],
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7zv7jzafHsH",
        "colab_type": "text"
      },
      "source": [
        "Let's see the Mean Squared Errors..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAtybVZ3Yq8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d040bb55-595f-483d-f889-3cdf970aaabc"
      },
      "source": [
        "print(f'MSE for train set is {np.round(mean_squared_error(y_train, d[\"Y_prediction_train\"]),2)} and MSE for test set is {np.round(mean_squared_error(y_test, d[\"Y_prediction_test\"]),2)}')"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE for train set is 5132743901.45 and MSE for test set is 5149507690.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxqXgg1aa7bN",
        "colab_type": "text"
      },
      "source": [
        "3. **Root Mean Squared Error (RMSE):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7HKsmAFZWDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "    return rmse"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H1BcOsgfWVG",
        "colab_type": "text"
      },
      "source": [
        "Let's see the Root Mean Squared Errors..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5nxm3OGbdQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f278307b-e05d-4abd-9efe-fb6f6061d9e5"
      },
      "source": [
        "print(f'RMSE for train set is {np.round(root_mean_squared_error(y_train, d[\"Y_prediction_train\"]),2)} and RMSE for test set is {np.round(root_mean_squared_error(y_test, d[\"Y_prediction_test\"]),2)}')"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE for train set is 71643.17 and MSE for test set is 71760.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNwAEZHmcAe2",
        "colab_type": "text"
      },
      "source": [
        "<b><h4>5.4 Compare the RMSE to the Range of the Target Variable</b></h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu1F3v2KcMMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse_range(y_true, y_pred):\n",
        "\n",
        "    # Find range of the target variable\n",
        "    target_range = np.ptp(y_true, axis=1)[0]\n",
        "\n",
        "    # Find RMSE\n",
        "    rmse = root_mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # RMSE as percent of target range\n",
        "    rmse_pct_range = (rmse / target_range) * 100\n",
        "\n",
        "    return rmse_pct_range"
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxlYsbgcco1b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f73c25e-2e3a-42b5-a471-4746999dc0e6"
      },
      "source": [
        "print(f'RMSE_target range for train set is {np.round(rmse_range(y_train, d[\"Y_prediction_train\"]),2)} and RMSE_target_range for test set is {np.round(rmse_range(y_test, d[\"Y_prediction_test\"]),2)}')"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE_target range for train set is 14.77 and RMSE_target_range for test set is 15.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tmpSBj7hF3n",
        "colab_type": "text"
      },
      "source": [
        "Next: Show a dist plot of the predicted values versus the real values for both training and testing sets. Use the seaborn library if possible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz9hSsmxhS-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}